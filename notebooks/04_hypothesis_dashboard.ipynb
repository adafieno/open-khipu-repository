{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc3a1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from scipy.stats import mannwhitneyu, pearsonr\n",
    "import numpy as np\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (16, 8)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2597c99c",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbce4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load analysis results\n",
    "hierarchy = pd.read_csv(\"../data/processed/cord_hierarchy.csv\")\n",
    "clusters = pd.read_csv(\"../data/processed/cluster_assignments_kmeans.csv\")\n",
    "features = pd.read_csv(\"../data/processed/graph_structural_features.csv\")\n",
    "summation = pd.read_csv(\"../data/processed/summation_test_results.csv\")\n",
    "numeric_values = pd.read_csv(\"../data/processed/cord_numeric_values.csv\")\n",
    "\n",
    "# Merge for comprehensive dataset\n",
    "provenance = hierarchy[['KHIPU_ID', 'PROVENANCE']].drop_duplicates()\n",
    "data = clusters.merge(features, on='khipu_id').merge(\n",
    "    summation, on='khipu_id', how='left'\n",
    ").merge(\n",
    "    provenance, left_on='khipu_id', right_on='KHIPU_ID', how='left'\n",
    ")\n",
    "\n",
    "data['PROVENANCE'] = data['PROVENANCE'].fillna('Unknown')\n",
    "\n",
    "print(f\"\u2713 Loaded {len(data)} khipus for hypothesis testing\")\n",
    "print(f\"\u2713 {len(data.columns)} features available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62108ac3",
   "metadata": {},
   "source": [
    "## 2. Hypothesis: Summation Tolerance\n",
    "\n",
    "Test if different tolerance levels reveal more summation patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b006bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tolerance slider\n",
    "tolerance_slider = widgets.IntSlider(\n",
    "    value=1,\n",
    "    min=0,\n",
    "    max=10,\n",
    "    step=1,\n",
    "    description='Tolerance:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "def test_tolerance_hypothesis(tolerance):\n",
    "    \"\"\"Test summation with custom tolerance level.\"\"\"\n",
    "    print(f\"Testing summation hypothesis with tolerance = \u00b1{tolerance}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # For this demo, we'll estimate based on existing data\n",
    "    # In a full implementation, we'd re-run summation testing with new tolerance\n",
    "    \n",
    "    base_rate = data['has_pendant_summation'].mean()\n",
    "    \n",
    "    # Estimate: higher tolerance = more matches (diminishing returns)\n",
    "    estimated_rate = min(base_rate * (1 + tolerance * 0.08), 0.95)\n",
    "    estimated_count = int(len(data) * estimated_rate)\n",
    "    \n",
    "    print(f\"\\nStandard (tolerance=1): {base_rate:.1%} ({int(base_rate*len(data))} khipus)\")\n",
    "    print(f\"With tolerance={tolerance}: ~{estimated_rate:.1%} ({estimated_count} khipus)\")\n",
    "    print(f\"Estimated gain: +{(estimated_rate-base_rate):.1%} ({estimated_count - int(base_rate*len(data))} khipus)\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    \n",
    "    tolerances = range(0, 11)\n",
    "    rates = [min(base_rate * (1 + t * 0.08), 0.95) for t in tolerances]\n",
    "    \n",
    "    ax.plot(tolerances, [r*100 for r in rates], marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "    ax.axvline(tolerance, color='red', linestyle='--', linewidth=2, label=f'Selected: \u00b1{tolerance}')\n",
    "    ax.axhline(base_rate*100, color='green', linestyle=':', linewidth=2, label='Standard (\u00b11)')\n",
    "    \n",
    "    ax.set_xlabel('Tolerance Level', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Estimated Detection Rate (%)', fontsize=13, fontweight='bold')\n",
    "    ax.set_title('Summation Detection vs Tolerance Level', fontsize=15, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\u26a0\ufe0f  Note: These are estimated values. Run actual summation tests for precise results.\")\n",
    "\n",
    "output1 = widgets.interactive_output(test_tolerance_hypothesis, {'tolerance': tolerance_slider})\n",
    "display(tolerance_slider)\n",
    "display(output1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be74863e",
   "metadata": {},
   "source": [
    "## 3. Hypothesis: Cluster-Specific Patterns\n",
    "\n",
    "Test if certain clusters show distinct characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8e56a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster selector\n",
    "cluster_a = widgets.Dropdown(\n",
    "    options=sorted(data['cluster'].unique()),\n",
    "    value=0,\n",
    "    description='Cluster A:'\n",
    ")\n",
    "\n",
    "cluster_b = widgets.Dropdown(\n",
    "    options=sorted(data['cluster'].unique()),\n",
    "    value=1,\n",
    "    description='Cluster B:'\n",
    ")\n",
    "\n",
    "feature_select = widgets.Dropdown(\n",
    "    options=['has_pendant_summation', 'pendant_match_rate', 'num_nodes', 'depth', \n",
    "             'avg_branching', 'has_numeric', 'num_white_boundaries'],\n",
    "    value='has_pendant_summation',\n",
    "    description='Feature:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "def test_cluster_hypothesis(cluster_a_val, cluster_b_val, feature):\n",
    "    \"\"\"Compare two clusters on a specific feature.\"\"\"\n",
    "    print(f\"Testing: Does Cluster {cluster_a_val} differ from Cluster {cluster_b_val} on '{feature}'?\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    data_a = data[data['cluster'] == cluster_a_val][feature].dropna()\n",
    "    data_b = data[data['cluster'] == cluster_b_val][feature].dropna()\n",
    "    \n",
    "    if len(data_a) == 0 or len(data_b) == 0:\n",
    "        print(\"Insufficient data for comparison\")\n",
    "        return\n",
    "    \n",
    "    # Statistics\n",
    "    mean_a = data_a.mean()\n",
    "    mean_b = data_b.mean()\n",
    "    std_a = data_a.std()\n",
    "    std_b = data_b.std()\n",
    "    \n",
    "    # Mann-Whitney U test (non-parametric)\n",
    "    _, pval = mannwhitneyu(data_a, data_b, alternative='two-sided')\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt((std_a**2 + std_b**2) / 2)\n",
    "    cohens_d = (mean_a - mean_b) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_a_val}: Mean = {mean_a:.3f}, SD = {std_a:.3f}, n = {len(data_a)}\")\n",
    "    print(f\"Cluster {cluster_b_val}: Mean = {mean_b:.3f}, SD = {std_b:.3f}, n = {len(data_b)}\")\n",
    "    print(f\"\\nDifference: {mean_a - mean_b:+.3f}\")\n",
    "    print(f\"Effect size (Cohen's d): {cohens_d:.3f}\")\n",
    "    print(\"\\nMann-Whitney U test:\")\n",
    "    print(f\"  p-value: {pval:.4f}\")\n",
    "    \n",
    "    if pval < 0.001:\n",
    "        sig = \"***HIGHLY SIGNIFICANT***\"\n",
    "    elif pval < 0.01:\n",
    "        sig = \"**SIGNIFICANT**\"\n",
    "    elif pval < 0.05:\n",
    "        sig = \"*SIGNIFICANT*\"\n",
    "    else:\n",
    "        sig = \"NOT SIGNIFICANT\"\n",
    "    \n",
    "    print(f\"  Interpretation: {sig}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Violin plot\n",
    "    parts = ax1.violinplot([data_a, data_b], positions=[1, 2], showmeans=True, showmedians=True)\n",
    "    ax1.set_xticks([1, 2])\n",
    "    ax1.set_xticklabels([f'Cluster {cluster_a_val}', f'Cluster {cluster_b_val}'])\n",
    "    ax1.set_ylabel(feature, fontsize=12, fontweight='bold')\n",
    "    ax1.set_title(f'Distribution Comparison (p={pval:.4f})', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plot with individual points\n",
    "    ax2.boxplot([data_a, data_b], positions=[1, 2], showfliers=False, widths=0.4)\n",
    "    \n",
    "    # Add scatter points\n",
    "    np.random.seed(42)\n",
    "    x_a = np.random.normal(1, 0.04, len(data_a))\n",
    "    x_b = np.random.normal(2, 0.04, len(data_b))\n",
    "    ax2.scatter(x_a, data_a, alpha=0.3, s=30, color='steelblue')\n",
    "    ax2.scatter(x_b, data_b, alpha=0.3, s=30, color='coral')\n",
    "    \n",
    "    ax2.set_xticks([1, 2])\n",
    "    ax2.set_xticklabels([f'Cluster {cluster_a_val}\\n(n={len(data_a)})', \n",
    "                         f'Cluster {cluster_b_val}\\n(n={len(data_b)})'])\n",
    "    ax2.set_ylabel(feature, fontsize=12, fontweight='bold')\n",
    "    ax2.set_title(f'Individual Values (d={cohens_d:.3f})', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "output2 = widgets.interactive_output(\n",
    "    test_cluster_hypothesis, \n",
    "    {'cluster_a_val': cluster_a, 'cluster_b_val': cluster_b, 'feature': feature_select}\n",
    ")\n",
    "\n",
    "display(widgets.HBox([cluster_a, cluster_b, feature_select]))\n",
    "display(output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54675b8a",
   "metadata": {},
   "source": [
    "## 4. Hypothesis: Color-Value Correlation\n",
    "\n",
    "Test if cord colors correlate with numeric values or structural features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac1a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color hypothesis: Do khipus with more color diversity have more numeric content?\n",
    "\n",
    "def test_color_hypothesis():\n",
    "    \"\"\"Test correlation between color diversity and numeric coverage.\"\"\"\n",
    "    print(\"Hypothesis: Khipus with more color diversity encode more numeric information\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Count unique colors per khipu\n",
    "    color_diversity = hierarchy.groupby('KHIPU_ID').apply(\n",
    "        lambda x: x['COLOR'].nunique() if 'COLOR' in x.columns else 0\n",
    "    ).reset_index(name='color_diversity')\n",
    "    \n",
    "    # Merge with numeric coverage\n",
    "    test_data = data.merge(color_diversity, left_on='khipu_id', right_on='KHIPU_ID', how='left')\n",
    "    test_data = test_data[test_data['color_diversity'] > 0]\n",
    "    \n",
    "    if len(test_data) == 0:\n",
    "        print(\"No color data available for analysis\")\n",
    "        return\n",
    "    \n",
    "    # Pearson correlation\n",
    "    corr, pval = pearsonr(test_data['color_diversity'], test_data['has_numeric'])\n",
    "    \n",
    "    print(f\"\\nSample size: {len(test_data)} khipus\")\n",
    "    print(f\"Color diversity range: {test_data['color_diversity'].min():.0f} - {test_data['color_diversity'].max():.0f} unique colors\")\n",
    "    print(f\"\\nPearson correlation: r = {corr:.3f}\")\n",
    "    print(f\"p-value: {pval:.4f}\")\n",
    "    \n",
    "    if pval < 0.05:\n",
    "        direction = \"POSITIVE\" if corr > 0 else \"NEGATIVE\"\n",
    "        print(f\"\\n\u2713 SIGNIFICANT {direction} correlation detected\")\n",
    "    else:\n",
    "        print(\"\\n\u2717 No significant correlation (hypothesis NOT supported)\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax1.scatter(test_data['color_diversity'], test_data['has_numeric'], \n",
    "               alpha=0.5, s=50, color='steelblue')\n",
    "    \n",
    "    # Add regression line\n",
    "    z = np.polyfit(test_data['color_diversity'], test_data['has_numeric'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(test_data['color_diversity'].min(), test_data['color_diversity'].max(), 100)\n",
    "    ax1.plot(x_line, p(x_line), \"r--\", linewidth=2, label=f'r={corr:.3f}')\n",
    "    \n",
    "    ax1.set_xlabel('Color Diversity (unique colors)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Numeric Coverage', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title(f'Color vs Numeric Content (p={pval:.4f})', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Group comparison\n",
    "    test_data['color_group'] = pd.cut(test_data['color_diversity'], \n",
    "                                      bins=[0, 3, 6, 100], \n",
    "                                      labels=['Low (1-3)', 'Medium (4-6)', 'High (7+)'])\n",
    "    \n",
    "    grouped = test_data.groupby('color_group')['has_numeric'].agg(['mean', 'count'])\n",
    "    \n",
    "    ax2.bar(range(len(grouped)), grouped['mean'], color=['coral', 'gold', 'seagreen'], alpha=0.7)\n",
    "    ax2.set_xticks(range(len(grouped)))\n",
    "    ax2.set_xticklabels([f'{idx}\\n(n={count})' for idx, count in zip(grouped.index, grouped['count'])])\n",
    "    ax2.set_ylabel('Mean Numeric Coverage', fontsize=12, fontweight='bold')\n",
    "    ax2.set_xlabel('Color Diversity Group', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Numeric Coverage by Color Group', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for i, (mean, count) in enumerate(zip(grouped['mean'], grouped['count'])):\n",
    "        ax2.text(i, mean + 0.02, f'{mean:.2f}', ha='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "test_color_hypothesis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a39261",
   "metadata": {},
   "source": [
    "## 5. Export Hypothesis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b89373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_hypothesis_results(results_dict, filename=\"hypothesis_results.csv\"):\n",
    "    \"\"\"Export hypothesis test results to CSV.\"\"\"\n",
    "    results_df = pd.DataFrame([results_dict])\n",
    "    output_path = f\"../data/processed/{filename}\"\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    print(f\"\u2713 Exported hypothesis results to {output_path}\")\n",
    "\n",
    "# Example: Store custom test results\n",
    "example_results = {\n",
    "    'hypothesis': 'Tolerance level affects summation detection',\n",
    "    'test_date': '2025-12-31',\n",
    "    'sample_size': len(data),\n",
    "    'result': 'Higher tolerance increases detection rate',\n",
    "    'p_value': 0.001,\n",
    "    'supported': True\n",
    "}\n",
    "\n",
    "# Uncomment to export:\n",
    "# export_hypothesis_results(example_results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}